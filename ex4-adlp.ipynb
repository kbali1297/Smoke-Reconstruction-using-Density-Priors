{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exercise 4: Smoke Reconstruction\n\nThe goal of this exercise is to reconstruct 2D smoke simulations from only the projected smoke density.\n\nThe observations consist of the projected smoke densities along $x$ and $y$ over 20 time steps and the reconstruction must contain the 2D smoke reconstructions as well as the velocity fields.","metadata":{"papermill":{"duration":0.004312,"end_time":"2022-06-28T07:37:37.37764","exception":false,"start_time":"2022-06-28T07:37:37.373328","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"!pip install phiflow\nfrom phi.flow import *\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nprint(torch.cuda.is_available())\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"papermill":{"duration":17.200575,"end_time":"2022-06-28T07:37:54.581585","exception":false,"start_time":"2022-06-28T07:37:37.38101","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-09T09:51:48.222886Z","iopub.execute_input":"2022-07-09T09:51:48.223824Z","iopub.status.idle":"2022-07-09T09:52:00.263555Z","shell.execute_reply.started":"2022-07-09T09:51:48.223758Z","shell.execute_reply":"2022-07-09T09:52:00.26248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Insert your reconstruction algorithm / network here, replacing the next cell.","metadata":{"papermill":{"duration":0.005084,"end_time":"2022-06-28T07:37:55.321772","exception":false,"start_time":"2022-06-28T07:37:55.316688","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"''' DATASET GENERATION FOR EXERCISE 4'''\nfrom phi.torch.flow import * \nfrom phi.field._grid import unstack_staggered_tensor\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:52:00.26549Z","iopub.execute_input":"2022-07-09T09:52:00.267756Z","iopub.status.idle":"2022-07-09T09:52:00.283286Z","shell.execute_reply.started":"2022-07-09T09:52:00.267718Z","shell.execute_reply":"2022-07-09T09:52:00.282276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pads centered grid tensor to staggered size\ndef cen_to_stag(centered_grid):\n    padded_values = math.pad(centered_grid.values, {'x':(0,1), 'y':(0,1)}, mode=math.extrapolation.ZERO)\n    return padded_values\n\n# reduces staggered size tensor to centered tensor\ndef reduce_to_cen(reduce_tensor):\n    return math.pad(reduce_tensor, {'x':(0,-1), 'y':(0,-1)}, mode=math.extrapolation.ZERO)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:52:00.284553Z","iopub.execute_input":"2022-07-09T09:52:00.284948Z","iopub.status.idle":"2022-07-09T09:52:00.296533Z","shell.execute_reply.started":"2022-07-09T09:52:00.284912Z","shell.execute_reply":"2022-07-09T09:52:00.295366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k=32 # number of discrete cells per dimsension\nNb = 100\nNt = 20\n# generate random buoyant smoke spheres \ndef generate_inflow(shape: math.Shape, bounds=Box(x=100, y=100)):\n    centers = math.random_uniform(shape, channel(vector='x,y')) * bounds.size * (0.5, .3) + (25,10.)\n    sizes =  math.random_uniform(shape) *4 + 3\n    strengths = 1\n    return CenteredGrid(SoftGeometryMask(Sphere(centers, radius=sizes)), 0, x=k, y=k, bounds=bounds) * strengths\n\n# dataset size in batch dimension\nINFLOW = generate_inflow(batch(batch=Nb) & instance(inflows=2)) \nvelocity = StaggeredGrid((0, 0), 0, x=k, y=k, bounds=Box(x=100, y=100))  # or CenteredGrid(...)\nsmoke = CenteredGrid(INFLOW, extrapolation.BOUNDARY, x=k, y=k, bounds=Box(x=100, y=100))\npressure = None\n\n# @math.jit_compile \ndef step(v, s, p, dt=5.):\n    s = advect.mac_cormack(s, v, dt) #+ INFLOW\n    buoyancy = s * (0, 0.1) @ v  # resamples smoke to velocity sample points\n    v = advect.semi_lagrangian(v, v, dt) + buoyancy * dt\n    v, p = fluid.make_incompressible(v, (), Solve('auto', 1e-5, 0))\n    return v, s, p\n\nsmokes, velocities, pxs, pys = [], [], [], []\nfor i in range(Nt):\n    velocity, smoke, pressure = step(velocity, smoke, pressure)\n    smokes.append(cen_to_stag(smoke).native(['batch','x','y']))\n    velocities.append(velocity.staggered_tensor().native(['batch', 'x', 'y','vector']))\n    px =torch.mean(smokes[-1], axis=1)\n    py =torch.mean(smokes[-1], axis=2)\n    pxs.append(px)\n    pys.append(py)\n\n# input data is stored with following dimensions: [batch, time, axis, (projection_x, projection_y)]\n# ouput data is stored with following dimensions: [batch, time, x, y, (smoke, vx, vy)]\nsmokes = torch.permute(torch.stack(smokes),[1,0,2,3])\nvelocities = torch.permute(torch.stack(velocities),[1,0,2,3,4])\npxs =  torch.permute(torch.stack(pxs),[1,0,2])\npys =  torch.permute(torch.stack(pys),[1,0,2])\ninput_data = torch.stack([pxs,pys], dim=-1).cpu().numpy()\noutput_data = torch.cat([torch.unsqueeze(smokes[:,:], -1), velocities[:,:]],-1).cpu().numpy()\n\nnp.save('training_input_ex4',input_data)\nnp.save('training_output_ex4',output_data)\nnp.savetxt('training_input_ex4.csv', input_data.flatten(), delimiter=',')\nnp.savetxt('training_output_ex4.csv', output_data.flatten(), delimiter=',')","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:52:00.299641Z","iopub.execute_input":"2022-07-09T09:52:00.300218Z","iopub.status.idle":"2022-07-09T09:52:35.075816Z","shell.execute_reply.started":"2022-07-09T09:52:00.300183Z","shell.execute_reply":"2022-07-09T09:52:35.074837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' SIMULATOR STEP BASED ON TENSORS '''\nfrom phi.field._grid import unstack_staggered_tensor\nstate =math.tensor(output_data[:10], batch('batch', 'time'), spatial('x,y'), channel(features='rho,vx,vy'))\nk=32\ndef cen_to_stag(centered_grid):\n    padded_values = math.pad(centered_grid.values, {'x':(0,1), 'y':(0,1)}, mode=math.extrapolation.ZERO)\n    return padded_values\n\ndef reduce_to_cen(reduce_tensor):\n    return math.pad(reduce_tensor, {'x':(0,-1), 'y':(0,-1)}, mode=math.extrapolation.BOUNDARY)\n\ndef step(state, dt=5.):\n    s = CenteredGrid(reduce_to_cen(state.features['rho']), extrapolation.BOUNDARY, x=k, y=k, bounds=Box(x=100, y=100))\n    unstacked_velocity = unstack_staggered_tensor(math.rename_dims(state.features['vx,vy'], 'features', 'vector'), extrapolation=math.extrapolation.ZERO)\n    v = StaggeredGrid(unstacked_velocity, 0, x=k, y=k, bounds=Box(x=100, y=100))\n    s = advect.mac_cormack(s, v, dt) #+ INFLOW\n    buoyancy = s * (0, 0.1) @ v  # resamples smoke to velocity sample points\n    v = advect.semi_lagrangian(v, v, dt) + buoyancy * dt\n    v,_= fluid.make_incompressible(v, (), Solve('auto', 1e-5, 0))\n    v = v.staggered_tensor()\n    return math.stack([cen_to_stag(s), v.vector['x'], v.vector['y']], channel(features='rho,vx,vy')) ","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:52:35.078577Z","iopub.execute_input":"2022-07-09T09:52:35.079432Z","iopub.status.idle":"2022-07-09T09:52:35.093706Z","shell.execute_reply.started":"2022-07-09T09:52:35.07939Z","shell.execute_reply":"2022-07-09T09:52:35.092831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CHECK IF RESULTS ARE THE SAME\nevolv = step(state.time[0])\nprint(math.abs(evolv-state.time[1]))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:52:35.095364Z","iopub.execute_input":"2022-07-09T09:52:35.095867Z","iopub.status.idle":"2022-07-09T09:52:35.69844Z","shell.execute_reply.started":"2022-07-09T09:52:35.095759Z","shell.execute_reply":"2022-07-09T09:52:35.697462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path to training_input_ex4.csv\ninput_file_path = \"./training_input_ex4.npy\"\noutput_file_path = \"./training_output_ex4.npy\"\ntest_file_path =  \"../input/advanced-dl-for-physics-assignment-4/test_input_ex4.npy\"\n# Loads training input, 100 training samples (batch), each sample is a 20-step time-series, axis is the spatial length (33 discrete point), and provides x&y projections\n#loaded_input = numpy.genfromtxt(input_file_path, delimiter=',').reshape(100,20,33,2)\nloaded_input = np.load(input_file_path).reshape(Nb,Nt,33,2)\nobservation = tensor(loaded_input, batch(batch=Nb,time=Nt), spatial(axis=33), channel(projection='x,y'))\n\n# Loads training labels, 100 training samples (batch), each sample is a 20-step time-series, 33x33 domain, contains smoke density (rho), and velocity (vx, vy)\n#loaded_output = numpy.genfromtxt(output_file_path, delimiter=',').reshape(100,20,33,33,3)\nloaded_output = numpy.load(output_file_path).reshape(Nb,Nt,33,33,3)\nlabel = tensor(loaded_output, batch(batch=Nb,time=Nt), spatial(x=33, y=33), channel(features='rho,vx,vy'))\n\nprint(observation)\nprint(label)\n\nloaded_test_input = np.load(test_file_path).reshape(-1,20,33,2)\nobservation_test = tensor(loaded_test_input, batch(batch=loaded_test_input.shape[0],time=20), spatial(axis=33), channel(projection='x,y'))\nprint(observation_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:52:35.699742Z","iopub.execute_input":"2022-07-09T09:52:35.700733Z","iopub.status.idle":"2022-07-09T09:52:35.738914Z","shell.execute_reply.started":"2022-07-09T09:52:35.700675Z","shell.execute_reply":"2022-07-09T09:52:35.737931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reconstruction = math.random_normal(observation.shape.batch, spatial(x=33, y=33), channel(features='rho,vx,vy'))\nreconstruction = math.random_normal(observation_test.shape.batch, spatial(x=33, y=33), channel(features='rho,vx,vy'))\n# must have shape (batch, time=20, x=33, y=33, features=rho,vx,vy)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.24479,"end_time":"2022-06-28T07:37:55.571476","exception":false,"start_time":"2022-06-28T07:37:55.326686","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-07-09T09:52:35.740422Z","iopub.execute_input":"2022-07-09T09:52:35.741016Z","iopub.status.idle":"2022-07-09T09:52:35.74939Z","shell.execute_reply.started":"2022-07-09T09:52:35.740978Z","shell.execute_reply":"2022-07-09T09:52:35.748249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(label.features['rho'])\nproj_rho_x = math.sum(label.features['rho'], dim='y')\nproj_rho_y = math.sum(label.features['rho'], dim='x')\nproj = proj_rho_x + proj_rho_y\n\n#model = u_net(2, 3, levels=4, filters=[4, 8, 8, 4], in_spatial=2)\nclass conv_net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.add_module(f'conv_1',nn.Conv2d(2, 4, 3, 1, 1) )\n        self.add_module(f'conv_2', nn.Conv2d(4, 4, 3, 1, 1))\n        self.add_module(f'conv_skip1', nn.Conv2d(2, 4, 1))\n\n        self.add_module(f'conv_3', nn.Conv2d(4, 8, 3, 1, 1))\n        self.add_module(f'conv_4', nn.Conv2d(8, 8, 3, 1, 1))\n        self.add_module(f'conv_skip3', nn.Conv2d(4, 8, 1))\n\n        self.add_module(f'conv_5', nn.Conv2d(8, 16, 3, 1, 1))\n        self.add_module(f'conv_6', nn.Conv2d(16, 16, 3, 1, 1))\n        self.add_module(f'conv_skip5', nn.Conv2d(8, 16, 1))\n        \n        self.add_module(f'conv_7', nn.Conv2d(16, 32, 3, 1, 1))\n        self.add_module(f'conv_8', nn.Conv2d(32, 32, 3, 1, 1))\n        self.add_module(f'conv_skip7', nn.Conv2d(16, 32, 1))\n        \n        self.add_module(f'conv_9', nn.Conv2d(32, 16, 3, 1, 1))\n        self.add_module(f'conv_10', nn.Conv2d(16, 16, 3, 1, 1))\n        self.add_module(f'conv_skip9', nn.Conv2d(32, 16, 1))\n        \n        self.add_module(f'conv_11', nn.Conv2d(16, 8, 3, 1, 1))\n        self.add_module(f'conv_12', nn.Conv2d(8, 8, 3, 1, 1))\n        self.add_module(f'conv_skip11', nn.Conv2d(16, 8, 1))\n        \n        self.add_module(f'conv_13', nn.Conv2d(8, 4, 3, 1, 1))\n        self.add_module(f'conv_14', nn.Conv2d(4, 4, 3, 1, 1))\n        self.add_module(f'conv_skip13', nn.Conv2d(8, 4, 1))\n\n        self.add_module(f'conv_15', nn.Conv2d(4, 3, 3, 1, 1))\n        self.add_module(f'conv_16', nn.Conv2d(3, 3, 3, 1, 1))\n        self.add_module(f'conv_skip15', nn.Conv2d(4, 3, 1))\n\n\n    def forward(self, x):\n        for i in range(1,15, 2):\n            x1 = F.relu(getattr(self, f'conv_{i}')(x)) \n            x = F.relu(getattr(self, f'conv_{i+1}')(x1) + getattr(self, f'conv_skip{i}')(x))\n        x1 = F.relu(getattr(self, 'conv_15')(x))\n        x = getattr(self, 'conv_16')(x1) + getattr(self, 'conv_skip15')(x) \n        return x \n\n\nmodel = conv_net().to('cuda')\n#model = U_net(2, 3, 6)\nprint('Model Parameter Count:')\nprint(sum(p.numel() for p in model.parameters() if p.requires_grad))\noptim = adam(model, learning_rate=1e-3)\ndef loss_fn(proj, u):\n    #proj = math.sum(u.features['rho'], dim='y') + math.sum(u.features['rho'], dim='x')\n    loss = 0\n    \n    for i in range(10):\n        #print(i)\n        proj = math.expand(math.rename_dims(proj, 'axis', 'axis_x') , spatial(axis_y=33))\n        pred = math.native_call(model, proj)\n        #print('proj_shape')\n        #print(proj.shape)\n        #print(pred.shape)\n        pred = math.tensor(pred, batch(batch=Nb), channel(features='rho,vx,vy'), spatial(x=33, y=33))\n        #print(pred)\n        loss += math.l2_loss(pred - u)\n        #u = step(u)\n        u = step(u)\n        #print(u_m.shape)\n        proj_x = math.sum(pred.features['rho'], dim='y')\n        proj_x = math.reshaped_native(proj_x, ('batch', 'x'))\n        proj_y = math.sum(pred.features['rho'], dim='x')\n        proj_y = math.reshaped_native(proj_y, ('batch', 'y'))\n        \n        proj_x = torch.unsqueeze(proj_x, 2)\n        proj_y = torch.unsqueeze(proj_y, 2)\n        proj = torch.concat([proj_x, proj_y], 2)\n        proj = math.tensor(proj,batch(batch=Nb), spatial(axis=33) , channel(projection='x,y'))\n    return math.sum(loss, 'batch')\n\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    for t in range(0,Nt):\n        proj = observation.time[t]\n        u = label.time[t]\n        #print('Training Loop Shapes:')\n        #print(proj.shape)\n        #print(u.shape)\n        loss = update_weights(model, optim, loss_fn, proj, u)\n        if t%5==0:\n            print(f'Epoch: {epoch}, Time Step: {t},Loss: {loss}')\n\n#reconstruction = math.native_call(model, observation_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T09:52:35.751236Z","iopub.execute_input":"2022-07-09T09:52:35.751846Z","iopub.status.idle":"2022-07-09T09:53:14.629354Z","shell.execute_reply.started":"2022-07-09T09:52:35.751807Z","shell.execute_reply":"2022-07-09T09:53:14.626834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nNbt = 1 #Test Batches\nproj = math.expand(math.rename_dims(observation_test.time[0], 'axis', 'axis_x') , spatial(axis_y=33))\n#print(proj.shape)\npred = math.native_call(model, proj)\n\npred = math.tensor(pred, batch(batch=Nbt), channel(features='rho,vx,vy'), spatial(x=33, y=33))\n\nreconstruction = pred\n#print(pred.shape)\nreconstruction = math.expand(reconstruction, batch(time=1))\nfor t in range(1,20):\n    proj = math.expand(math.rename_dims(observation_test.time[t], 'axis', 'axis_x') , spatial(axis_y=33))\n    pred = math.native_call(model, proj)\n    \n    pred = math.tensor(pred, batch(batch=Nbt), channel(features='rho,vx,vy'), spatial(x=33, y=33))\n    pred = math.expand(pred, batch(time=1))\n    \n    reconstruction = math.concat( [reconstruction, pred], 'time')\n\nprint(reconstruction.shape)\nreconstruction = math.reshaped_native(reconstruction, ('time', 'batch', 'x', 'y', 'features'))#batch(time=20, batch=1),spatial(x=33, y=33), channel(features='rho,vx,vy'))\nreconstruction = torch.permute(reconstruction, [1, 0, 2,3,4])\nreconstruction = math.tensor(reconstruction, batch(batch=1, time=20), spatial(x=33, y=33), channel(features='rho,vx,vy'))\nprint(reconstruction.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T00:24:14.610238Z","iopub.status.idle":"2022-07-09T00:24:14.613752Z","shell.execute_reply.started":"2022-07-09T00:24:14.613472Z","shell.execute_reply":"2022-07-09T00:24:14.613499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Do not edit the following cell!**\nFinally, we compute the physics and projection match loss. The following cell must execute last in your notebook.","metadata":{"papermill":{"duration":0.006476,"end_time":"2022-06-28T07:37:55.586751","exception":false,"start_time":"2022-06-28T07:37:55.580275","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"assert reconstruction.shape == observation_test.shape.batch & spatial(x=33, y=33) & channel(features='rho,vx,vx'), \"Reconstruction does not have the correct shape!\"\n\nfrom phi.field._grid import unstack_staggered_tensor\ndef cen_to_stag(centered_grid):\n    padded_values = math.pad(centered_grid.values, {'x':(0,1), 'y':(0,1)}, mode=math.extrapolation.ZERO)\n    return padded_values\n\ndef reduce_to_cen(reduce_tensor):\n    return math.pad(reduce_tensor, {'x':(0,-1), 'y':(0,-1)}, mode=math.extrapolation.BOUNDARY)\n\ndef step(state, dt=5.):\n    s = CenteredGrid(reduce_to_cen(state.features['rho']), extrapolation.BOUNDARY, x=32, y=32, bounds=Box(x=100, y=100))\n    unstacked_velocity = unstack_staggered_tensor(math.rename_dims(state.features['vx,vy'], 'features', 'vector'), extrapolation=math.extrapolation.ZERO)\n    v = StaggeredGrid(unstacked_velocity, 0, x=32, y=32, bounds=Box(x=100, y=100))\n    s = advect.mac_cormack(s, v, dt) #+ INFLOW\n    buoyancy = s * (0, 0.1) @ v  # resamples smoke to velocity sample points\n    v = advect.semi_lagrangian(v, v, dt) + buoyancy * dt\n    v,_= fluid.make_incompressible(v, (), Solve('auto', 1e-5, 0))\n    v = v.staggered_tensor()\n    return math.stack([cen_to_stag(s), v.vector['x'], v.vector['y']], channel(features='rho,vx,vy'))\n\nphys_loss = math.l2_loss(step(reconstruction).time[:-1] - reconstruction.time[1:]).numpy('batch,time').flatten()\nproj_x = math.sum(reconstruction.features['rho'], 'x').numpy('batch,time,y').flatten()\nproj_y = math.sum(reconstruction.features['rho'], 'y').numpy('batch,time,x').flatten()\nnumpy.savetxt('result.csv', numpy.concatenate([phys_loss, proj_x, proj_y]), delimiter=',')\n\n\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.814469,"end_time":"2022-06-28T07:37:56.406704","exception":false,"start_time":"2022-06-28T07:37:55.592235","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[],"execution":{"iopub.status.busy":"2022-07-09T00:24:14.617658Z","iopub.status.idle":"2022-07-09T00:24:14.618452Z","shell.execute_reply.started":"2022-07-09T00:24:14.618174Z","shell.execute_reply":"2022-07-09T00:24:14.618198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = np.concatenate([phys_loss, proj_x, proj_y], axis = 0)\nprint(result.shape)\nsubmission_df = pd.DataFrame(data = {'Id':np.arange(result.shape[0]), 'Expected':result})\nsubmission_df.to_csv('result.csv', index=False)","metadata":{"papermill":{"duration":0.026285,"end_time":"2022-06-28T07:37:56.438333","exception":false,"start_time":"2022-06-28T07:37:56.412048","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-09T00:24:14.619855Z","iopub.status.idle":"2022-07-09T00:24:14.627025Z","shell.execute_reply.started":"2022-07-09T00:24:14.626671Z","shell.execute_reply":"2022-07-09T00:24:14.6267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.00485,"end_time":"2022-06-28T07:37:56.4488","exception":false,"start_time":"2022-06-28T07:37:56.44395","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}